> * 翻译自俄语文章：https://habr.com/ru/articles/789382/
> * STM32F407VGT6 项目：https://github.com/iliasam/STM32F4_SDR_GPS
> * ESP32 项目：https://github.com/iliasam/ESP32_SDR_GPS
> * 信号处理算法基于 GNSS-SDRLIB：https://github.com/taroz/GNSS-SDRLIB
> * 数据处理算法基于 RTKLIB：https://github.com/tomojitakasu/RTKLIB

> **文章介绍**：
>
> 

# 基于 MCU 的 GPS 软件接收机

在这篇文章中，我将介绍如何用微控制器自制 SDR GPS 接收机。在这种情况下，SDR 意味着接收器不包含现成的 GPS 模块或处理 GPS 信号的专用芯片 - 所有 "原始 "数据的处理都在微控制器（STM32 或 ESP32）上实时完成。

我为什么这样做？ - 只是为了好玩，另外 - 积累经验。

[TOC]

## 前言

这篇文章在许多方面都是我上一篇文章的延续：《[旧 GPS 接收机的新生](https://habr.com/ru/articles/765402/)》。在那篇文章中，我简要介绍了 GPS 接收机的原理，并描述了我是如何获取 GPS "原始 "数据并在电脑上进行处理的。文中描述的原理适用于所有类型的 GPS 接收机，主要区别在于处理信号的方式。在个人电脑上处理数据是一种非常特殊的情况，消费者通常希望接收器尽可能小巧、低功耗。

考虑到 GPS 数据处理需要同时对多个信道进行大量实时数学运算，使用定制芯片和专用集成电路（ASIC）似乎是完成这项任务的理想解决方案。实际情况正是如此--绝大多数现代 GPS 接收器都使用专用芯片。这种原理已经使用了很长时间。

一些老式 GPS 接收机（如索尼 [Pyxis IPS-360](http://retro-gps.info/Sony/Sony-Pyxis-IPS-360/index.html)）可能具有这种结构图：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310184735579.png" alt="image-20240310184735579" style="zoom: 67%;" />

当时的各个模块都是独立的芯片。在这种设计中，首先是放大器和滤波器。然后是全球导航卫星系统射频前端--它将无线电信号传输到较低的频率并将其数字化（这里的 ADC 比特率为 1-2 比特）。然后是上述用于 GPS 数据处理的专用芯片，有时也称为相关器，因为它的主要任务是找到每个接收卫星的最大相关性。GP2021 就是这种芯片的一个例子。接下来是一个典型的微控制器或微处理器，它处理来自相关器的相对缓慢变化的数据，并计算接收器的位置。随着技术的发展，所有这些模块都可以集成到一个芯片中。这种芯片主要用于 GPS 接收机模块，DIY 爱好者们已经对此司空见惯。

如果不想使用现成的芯片，但又想处理原始数据处理，该怎么办呢？最简单的方法是使用射频前端 + FPGA，在其中使用 ADC 实现所有资源密集型数据处理，用户坐标可在同一 FPGA 上读取（使用软件处理器）或使用外部处理器。这个 DIY 项目[1]就采用了这种方法。

有没有可能不使用 FPGA 和 PC，直接在 STM32 等微控制器上实时处理原始数据？老实说，我找不到这样的项目。DSP 有这样的项目，但作者使用的是功能强大的处理器和大量的 RAM。

有这样一个奇特而复杂的微型消费 GPS 接收器项目 - （[制造商网站](https://insidegnss.com/what-is-snapshot-positioning-and-what-advantages-does-it-offer/)）。开发人员利用一些技巧，实现了确定坐标的能力，包括以毫秒为单位、以数百秒为周期的接收器。同时，他们没有使用 GPS 接收机的传统跟踪功能（跟踪来自卫星的信号）。甚至不接收导航数据！所有数据处理均由 MAX32632 微控制器（Cortex-M4）完成。因此，我必须亲自了解这项任务是否可行。

## 获取 "原始 "GPS 数据

我在上一篇文章中已经介绍了很多。这次我决定使用我已经自制的基于 MAX2769 芯片的 GNSS 射频前端调试板。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310200930600.png" alt="image-20240310200930600" style="zoom: 67%;" />

该芯片之所以方便，是因为它有一个特殊模式--预配置设备状态，只要在 PGM 输入端输入 1，就能将其切换到该模式。这样，无需通过 SPI 配置，就能从芯片获取数据。我使用的是配置选项 2。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310201039166.png" alt="image-20240310201039166" style="zoom:67%;" />

射频前端方案：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310201206159.png" style="zoom: 50%;" />

实际上，整个电路板由 MAX2769、16.368 MHz 石英振荡器 TCXO 和 3.0V 线性电压源组成。
有源天线连接到连接器 J3，时钟信号（也是 16.368 MHz）来自连接器 P5。电路板通过连接器 P3 输出数字化数据--"原始 "GPS 数据。Gerver 文件已发布到[存储库](https://github.com/iliasam/STM32F4_SDR_GPS)中。

## 在 STM32 上采集数据

因此，我们在 MAX2769 芯片的输出端看到的是两比特数据流。不过，单比特数据对于接收器的运行已经足够，因此为了简化设计，我采用了这种模式。数据取自 MAX2769 的 I1 引脚（符号）。在实验中，我决定使用一块旧的 STM32F4-Discovery 调试板。自 2012 年以来，我就没有再用它做过任何事情，但它的原生微控制器仍然健在）。

MCU 特性：

* Cortex-M4 内核，32 位
* 主频：168MHz
* Flash：1 Mbyte
* RAM：192 Kbytes

原来，MCU 必须以 ~16Mbit/s （2 Mbyte/s）的速度捕获串行同步数据流。这是一个很大的数字。SPI 接口与 DMA 结合使用最适合这一目的。DMA 配置为循环模式，即当到达分配存储器的末端时，DMA 将自动开始写入存储器的起点。这样，我们就能在存储器中获得一组数据，其中一个比特对应 MAX2769 ADC 的一个采样。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310201517842.png" alt="image-20240310201517842" style="zoom:67%;" />

在 RAM 中分配了一个缓冲区来存储接收到的数据，但当接收到的数据计数器到达内存的中间和末端时，DMA 可以产生中断。这样就可以提供双重缓冲--内存的一部分被填满，第二部分就会被处理。请注意，在接收过程中，不能停止接收，甚至不能丢失单个字节，否则将导致同步丢失。在接收开始时，有必要执行较长的操作，因此在执行此类操作之前，应尽快将新数据保存到额外的缓冲区中。

## 频率、数据和时间

MAX2769 开发人员选择 16.368 MHz 的时钟信号频率是有原因的。回想一下，GPS 系统使用重复范围码 (PRN)，卫星以 1 毫秒的周期发射这种信号。对于 1 毫秒的时间，芯片 MAX2769 将产生 16368 个 ADC 采样。同时，一个 PRN 周期包括 1023 个码片（信号调制相位不变的单段）。16368 / 1023 = 16，即一个码片正好对应 16 个 ADC 采样。在我的例子中，正好是 16 位 = 2 个字节。事实证明，一个字节 = 0.5 个码片，这在未来将非常有用。

事实证明，必须分配 16368/8= 2046 字节（约 2 Kbytes）才能存储 1 毫秒的数据（1 PRN）。双缓冲需要约 4 千字节，另外还需要约 2 千字节的缓冲区用于长数据处理。在所选模式下，MAX2769 以 4.092 MHz 的中间频率 (IF) 输出数据。这正好是时钟频率的 1/4。事实证明，在完全没有干扰的情况下，单个码片的输出结果会像 0b11001100110011001100（这是现有的 4 种移相变量之一）。

## 数据处理

在上一篇文章中，我已经介绍了在 PC 上处理数据的方法。显然，MCU 性能太弱，无法直接从 PC 传输算法--它缺乏内核性能、RAM 容量，甚至没有 PC 上可用的特殊指令。

当 PC 接收到来自 GNSS 射频前端的数据流时，通常会做什么？将每个 ADC 样本转换为有符号整数格式，甚至浮点格式。这种方法意味着处理器每秒要处理 16368000 个采样。这是一个很大的数字，所以我甚至懒得去检查这是否符合实际情况，即使考虑到 STM32F4 也被视为 DSP 并具有 FPU。

相反，我采用了在这个项目中看到的另一种方法：[自制 GPS 和 GLONASS 卫星接收器](https://lea.hamradio.si/~s53mv/navsats/theory.html) [2]。作者用普通的数字芯片制作了一个 GPS 接收机。当然，那里使用的方法不能直接移植到单片机上，但有些东西是可以借鉴的。最主要的是，那里对数字数据的部分操作是在单比特级别上进行的。DSP 的关键运算乘法被逻辑运算 "Exclusive OR"（XOR）所取代。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310201757427.png" alt="image-20240310201757427" style="zoom:50%;" />

如果我们看一下反相 XOR 运算（XNOR）的真值表，并想象值 "0 "对应负号信号"-1"，那么就会发现 XNOR 确实可以取代乘法运算（不带进位）。两个连续的 XOR 通过反转相互补偿，因此在这种情况下不需要 XNOR。XOR 操作之所以好，是因为它可以同时对大量采样位进行操作，对 16/32 位字进行操作，而且速度非常快。那么，GPS 接收机应该进行哪些操作呢？这就是经典的操作链结构图：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310201845369.png" alt="image-20240310201845369" style="zoom:50%;" />

左半部分是信号从一个频率到另一个频率的经典传输。回想一下，从卫星接收到的信号有多普勒频率偏移，每颗卫星都会有自己的频率偏移。此外，接收器的时钟振荡器也可能有自己的频率误差，MAX2769 不是将信号转移到零，而是转移到一个中间频率。通过将原始信号与所需频率（载波 NCO）的谐波信号相乘，我们将在乘法器的输出端得到一个新信号--输入信号将被[下变频](https://ru.wikipedia.org/wiki/%D0%A1%D0%BC%D0%B5%D1%81%D0%B8%D1%82%D0%B5%D0%BB%D1%8C_(%D1%8D%D0%BB%D0%B5%D0%BA%D1%82%D1%80%D0%BE%D0%BD%D0%B8%D0%BA%D0%B0))到一个较低的频率。在 GPS 中，通常会立即将信号转换到零频率，即在乘法器之后得到发射机发射的信号。

但是，振荡器上方/下方的信号也会出现向下偏移的现象，无法相互分离。为了解决这个问题，需要使用两个混频器（乘法器）来形成两个偏移 90 度的谐波信号。事实上，在这种情况下，不仅信号被转移到另一个频率，而且还被转换成复数形式，从而提供了有关信号相位的额外信息。由此产生的信号为[正交信号](https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%BD%D1%84%D0%B0%D0%B7%D0%BD%D0%B0%D1%8F_%D0%B8_%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D1%83%D1%80%D0%BD%D0%B0%D1%8F_%D1%81%D0%BE%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D1%8F%D1%8E%D1%89%D0%B8%D0%B5_%D1%81%D0%B8%D0%B3%D0%BD%D0%B0%D0%BB%D0%B0)，其分量用字母 I 和 Q 表示。

下一步是生成本地 PRN 码。每个卫星的代码都不同，可在接收机初始化时生成。代码本身由 1023 个码片/比特组成。但是，不能使用直接生成的数据，在乘法之前必须转换数据速率，使其与 SPI 数据的速率相匹配。

下一步是数据的最终乘法。事实证明，一方面我们有从卫星接收并传输到零频率的 1ms 数据，另一方面我们有本地的 "理想 "PRN 数据（本地复制代码）。将数据元素逐个相乘，然后求和，我们就得到了信号相似度的测量值，或者说是相互关联的结果。从该方案中可以看出，对于每个 I/Q 信道，我们都需要进行不同的计算。

这里值得介绍一个经常提到的概念--PRN 码的相位。事实上，它只是本地 PRN 数据为最大限度地提高相关性而必须移位的延迟量。卫星在不断移动，因此编码的相位也在不断变化。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310202044038.png" alt="image-20240310202044038" style="zoom:67%;" />

如果没有干扰，中间频率发生器载波 NCO 的频率和相位与接收信号的频率和相位完全重合，且接收到的数据代码的相位等于本地生成的 PRN 的相位，那么在卫星传输设置为 "1 "位的导航数据时，相关器的输出将达到最大值，而在传输 "0 "位时，相关器的输出将达到最小值。回想一下，一个比特的数据传输时间为 20 毫秒，在此期间会连续传输 20 个 PRN 编码，在传输 "0 "期间，传输的码片会被反转。

## 算法在实际应用中的特殊性

如上文所述，在我的实现过程中，所有输入数据都是以 1 毫秒为单位进行处理的，而为了进行跟踪，数据必须连续处理。这意味着上述所有计算都必须在 1 毫秒内完成。

显然，在这种情况下，使用三角函数计算单个比特值是非常浪费的。你可以看到 [DDS 振荡器](https://ru.wikipedia.org/wiki/%D0%A6%D0%B8%D1%84%D1%80%D0%BE%D0%B2%D0%BE%D0%B9_%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9_%D1%81%D0%B8%D0%BD%D1%82%D0%B5%D0%B7%D0%B0%D1%82%D0%BE%D1%80)是如何产生频率的。在那里，一切都非常简单，可以用整数形式实现。由于生成的是单比特数据，因此甚至不需要查找表（LUT），只需检查相位累加器变量的高比特即可。然而，事实证明，即使是这种简单的方法也过于耗时。计算 16368*2 位数值需要 1280 微秒。

因此，我决定采用一种略有不同的方法，其依据是所产生的频率非常接近时钟频率的 1/4。在这个项目中，我将多普勒极限偏移设置为 7kHz，这意味着所需的极限频率偏移约为 0.2%。事实证明，生成信号的周期为 4 个采样点（比特），频率偏移只会造成生成信号的相移。

因此，要使缓冲区充满所需频率的数据，只需将其分割成 32 位字，并将表中的值写入其中即可：

const uint32_t sin_buf32[4] = { 0x333333333333, 0x999999999, 0xCCCCCCCCCCCC, 0x66666666 }；

表中的索引只是相位值，应定期更改，计算相位的方法与 DDS 振荡器相同。如果看一下上面的数据接收算法结构图，就会发现载波 NCO 发生器数据在其他任何地方都没有被使用。事实证明，将数据填入一个特殊的缓冲器是毫无意义的--可以通过 XOR 将来自 SPI 的数据与 sin_buf32 元素相乘，并将结果保存到中间缓冲器中。正交数据的第二部分也是如此。由此产生的 I/Q 数据函数执行时间为 44 µs。

在这里，一切都变得明显简单。PRN 数据已在初始化过程中生成，只需将其提升到 SPI 速度即可。一个 PRN 码片持续 16 个采样（位），因此只需将缓冲区分成 16 位字，并使用原始 PRN 数据填充 0x0/0xFFFF 值即可。在这一阶段，您也可以以 1 位 = 1/16 码片的增量移动要写入的数据，但在这种情况下，您需要使用 32 位字。结果函数的执行时间为 70 µs。

此时会出现一个重要的细微差别--如何处理代码阶段？

只有当本地 PRN 复制品的码相位与接收数据的码相位完全一致时，相关器才能产生有意义的结果。事实证明，必须能够控制数据的移位。我决定在乘法运算时进行移位，同时只移位处理数据的指针。指针至少可以移动一个字节，这意味着代码相位设置的精度为 0.5 码片。

只是数据缓冲区只有 1 毫秒，移位肯定会导致带数据的数组出界，如何解决这个问题呢？更正确的方案是改变数据处理方法，但我决定走简单路线。如上所述，传输一个导航位需要 20 毫秒，这意味着 PRN 数据要连续重复 20 次。这样就可以循环进行数据处理：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310202315780.png" alt="image-20240310202315780" style="zoom:67%;" />

原来，在这种情况下，先处理新的 PRN 序列数据，然后再处理旧的 PRN 序列数据。如果此时传输的数据没有变化，算法的运行就没有问题。但当导航数据的符号发生变化时，相关结果将直接取决于数据的符号和编码相位的值。这就很难确定导航数据符号发生变化的确切时刻，并极大地干扰了信号采集（Acquisition）过程。

求和操作仍然存在。要计算相关结果，我们需要将乘法运算产生的所有比特相加。而这些位有很多 - 16368 个。遗憾的是，所使用的微控制器没有内核命令允许计算设置位的总和（在 x86 中为 popcount）。必须完全通过编程实现求和。我研究了各种方法，最后决定使用一个简单的 16 位查找表。它存储在 RAM 中（因此速度更快），占用 64 Kbytes。该表在初始化时填入。由于求和运算是 16 位的，因此 XOR 乘法运算也是 16 位的。改用 32 位计算并不能显著提高速度，因为求和运算需要更多时间。在这个阶段，1 毫秒内累积的比特流变成了一对 int16_t 值 I/Q。因此，I/Q 数据的乘法和加法运算只需 112 微秒。

## 接收机管理

为了使上述接收算法正常工作，必须知道以下信号参数：

* 生成 PRN 数据的卫星编号
* 卫星频率的多普勒频移，以将接收到的信号正确移至零频率
* 码相位
* 载波相位（载波 NCO 相位）

在我的项目中，我手动指定了卫星编号，而在成品设备中，相关通道非常多，在 "冷启动 "的情况下，可以在短时间内轻松检查所有卫星的信号是否存在。

为了确定信号代码的当前频率和相位，接收机通常有一个特殊模式 - 采集。由于微控制器的资源太少，这种模式耗时最长。我将其分为几个阶段--频率搜索和多次迭代的代码相位搜索。在我的例子中，采集不是使用实时数据，而是使用复制到附加缓冲器中的数据。

## 频率搜索

采集中最长的部分。在上一篇文章中，我提到过一种基于 FFT 的频率和相位搜索特殊方法。但是，对于单片机来说，16368 点的 FFT 是一件相当耗费资源的事情，我没有找到任何专门的单比特 FFT。因此，在这个项目中，我通过枚举所有参数直接实现了搜索。

主要操作是找到最大相关性。您需要搜索代码阶段的所有 2046 个变体并找到最大值（理想情况下搜索所有 16368 个样本，但时间太长）。这样的搜索大约需要 0.23 秒。然而，输入信号的噪声相当大，再加上我的环形相关器有可能在导航比特发生变化的瞬间被击中--分析这样的数据毫无用处，但在采集过程中检测这样的事件却很成问题。因此，我连续运行了 10 次搜索（每次都使用最新的实际数据），并将找到的代码阶段数据保存到一个数组中。通过检查该数组中是否有经常出现的值，我们可以判断该频率是否有来自卫星的信号。这种方法有一个问题--如果卫星相对于观测者快速移动，代码相位就会快速变化，这样就很难分析这些数据。

通过在上述过程结束时扫描频率并分析得到的直方图，可以提前了解发生传输的频率。在我的项目中，我以 500 Hz 的步长扫描频率范围 [-7000...+7000 Hz]。整个范围共 29 步，完全扫描大约需要 60 秒。如果卫星信号较弱，则需要重复扫描。请注意，所有这些都是针对单颗卫星的，因此每使用一颗卫星也需要如此长的搜索时间。最终可能需要 4-10 分钟才能找到 4 颗卫星的频率。

## 码搜索

在这一阶段，接收机必须确定当前的码相位。这里的搜索原理与前面使用的类似，但找到的码相位会立即添加到直方图中，并立即对直方图进行分析。
如果在直方图中检测到一个较大的峰值，则认为搜索结束。问题是搜索所有相位需要很长时间，在搜索其他卫星的相位时，一个相位可能会 "逃脱"。在信号不是很强的情况下，情况就是这样（斜线是正确找到的代码相位值，其他值是噪声）：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310202633043.png" alt="image-20240310202633043" style="zoom:67%;" />

利用频率偏移数据预测代码相位偏离率是完全可能的（这些参数之间存在联系），但我采用了一种更简单的方法--迭代法。也就是说，在第一阶段扫描所有相位（2046 步），在下一阶段扫描 +-250 步（从找到的相位开始计算），然后再扫描 -60 步。随着扫描步数的减少，搜索速度也会相应提高。在第一阶段必须设置超时--如果直方图中在一定时间内没有发现峰值，则应重置超时，否则移位阶段会在直方图上 "涂抹"。最终，可以找到所有卫星的代码相位，但仍然无法达到所需的精确度。

## 信号跟踪

这部分算法已经严格实时执行。由于在我的案例中，代码阶段从一开始就找得不够准确，因此我在这里增加了一个阶段--预跟踪。与获取过程不同，它也必须实时执行。预跟踪算法也是以搜索最大相关性为基础的，只是搜索操作的次数有限制--必须在 1 毫秒内完成。这里的数据分析原理与采集过程相同--代码阶段的新值被添加到数组中，在此过程中对相同值进行分析。多个相同值有很大可能是正确的码相位值。这样就能实时获得精度为 0.5 码片的代码相位值，然后我们就可以继续进行真正的信号跟踪操作了。在许多方面，我在上一篇文章中也描述了这一过程。

开始跟踪时，信号的频率大致已知，载波频率的相位未知，接收到的编码相位随时可能 "跑掉"。这就是接收机必须 "监控 "所有这些参数的原因。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310202739668.png" alt="image-20240310202739668" style="zoom:67%;" />

上图包含前面提到的信号处理元件--载波 NCO、PRN 码发生器、乘法器和加法器，只是现在它们受相关计算所获得数据的控制。您可以看到，这里使用了两个反馈回路来控制参数 - DLL（延迟锁定回路）和 PLL/FLL（锁相回路/频率锁定回路）。我从 GNSS-SDRLIB 软件中提取了这两种算法的实现方法。

细心的读者可能已经注意到，上图中有三个相关器。它们处理的数据相同，但输入的代码相位值不同，相差 0.5 个码片（-1/2；0；+1/2）。这种方法被称为（早期、提示、晚期），通过这种方法可以估算出程序码相的当前值与接收到的值相差多少，包括误差符号。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310202848474.png" alt="image-20240310202848474" style="zoom:67%;" />

由于在我的项目中，1 个字节的数据只相当于 0.5 个码片，因此三个移位选项的实现非常简单。不过，这种方法也有缺点--如果误差因某种原因大于 0.5 码片，反馈机制可能会永久性地 "丢失 "信号。

DLL 环路的任务是使用 E-P-L 相关器数据跟踪信号代码的相位。在我的特殊情况下，以及在 GNSS-SDRLIB 中，只使用 E-L 相关器数据。误差的计算公式如下：
$$
\frac{\left(I_{E}^{2}+Q_{E}^{2}\right)-\left(I_{L}^{2}+Q_{L}^{2}\right)}{\left(I_{E}^{2}+Q_{E}^{2}\right)+\left(I_{L}^{2}+Q_{L}^{2}\right)}
$$
然后对误差值进行过滤，再乘以一个附加系数，用来改变当前的编码相位值。

基于 XOR 乘法的相关器能以 1 字节=0.5 码片的精度改变代码相位。如果想要更精确呢？提高代码相位设置的精度可以改善相关结果，更准确地确定距离。这就需要使用 PRN 数据发生器将数据移位一位的功能。

PLL/FLL - 用于控制载波 NCO 的频率。通过改变频率，还可以控制载波 NCO 信号的相位。要从信号中提取导航数据，就必须进行相位跟踪。在 GPS 中，相位跟踪通常使用 Costas Loop 实现。这种控制器很方便，因为它对信号相位的 180 度变化并不敏感，只是在传输的导航数据符号发生变化时才会出现。该调节器的任务是保持通道 I 的振幅最大，Q-最小。初始数据是相关器 P（提示）给出的结果。频率相位误差的计算公式为：
$$
D=\tan ^{-1}\left(\frac{Q^{k}}{I^{k}}\right)
$$
该误差值还经过滤波，乘以一个附加系数，用于改变当前的接收频率偏移值。

FLL 频率调整基于频率相位误差的时间导数。我在算法实施过程中注意到，在某些情况下，PLL/FLL 会 "捕捉 "到错误的频率。为了解决这个问题，我在代码中增加了对接收数据的检查。如果数据不正确，频率会自动调整为随机频率，然后 FLL 会自动尝试调整频率。很有可能我只是在代码中的某个地方出了错，导致了这种情况。

为了在开发过程中评估反馈状态，最好有一些工具能让您直观地看到跟踪状态。在我看来，一个好的解决方案可能是关联器 P（提示）结果的分布图。关联器输出 I/Q 值，I 值在 X 轴上绘制，Q - 在 Y 轴上绘制。我是在 PC 上调试算法工作的，因此显示这样的图表不成问题。

退出模式后，DLL/PLL 正常工作，得到下图（BPSK 调制的信号星座清晰可见）：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310203103026.png" alt="image-20240310203103026" style="zoom:50%;" />

如果出现 PLL 相位误差，就会发生星座旋转：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310203120048.png" alt="image-20240310203120048" style="zoom:50%;" />

如果存在强烈的相位误差（PLL 失相或未能实现相位稳定），而 DLL 工作正常，则图像看起来像一个圆：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310203151991.png" alt="image-20240310203151991" style="zoom:50%;" />

如果 DLL 工作不正常，代码阶段走得太远，那么图表中心将只有一个点，其大小仅由噪声决定。

## 多通道复用

如果我们计算一下处理来自一颗卫星的新数据包所需的总时间：频率传输（44 微秒）+ PRN 形成（70 微秒）+ 相关器（112 微秒 x 3）= 450 微秒。可以看出，这对于两颗卫星来说是远远不够的，但对于 4 颗卫星来说--没门。如果数据处理不能中断，该怎么办？

在文章[2]中找到了解决办法--那里所有相关节点都是硬件，接收器是单通道的。作者在文章中使用了多路复用信道--接收器定期在各个卫星之间切换。满足以下条件非常重要：

* 同步（通过 SPI 接收数据）不得受到任何干扰。
* 单颗卫星必须定期接收多个连续的 1 毫秒窗口，以分析导航数据。要确定坐标，必须知道导航数据符号变化的确切时刻，为此必须分析几个连续的 PRN。
* 您不能为一颗卫星分配太多时间 - 必须接收所有导航比特，而一个比特持续 20 毫秒。此外，如果切换频道的频率过低，又不进行跟踪，接收到的信号参数可能会有很大偏移。
* 20 毫秒的导航比特传输时间不应被多路复用时间所分割，否则可能会出现某个频道不再接收信号变化的情况。

在项目[2]中，作者遇到了自己的困难--由于接收器的硬件特性，信道切换本身需要 1 毫秒，因此不可能频繁切换信道。因此，他不得不采用一种相当复杂的算法，用一个特权信道和三个 "简单 "信道接收数据。只有从特权卫星才能接收导航数据，而且必须定期更换特权信道。在我的案例中，频道切换完全是通过编程完成的，因此几乎不需要任何时间。

经过各种计算，我决定采用以下方法：4 个信道分配给 4 颗卫星，每个接收信道分配 4 毫秒，在接收完所有卫星的数据（16 毫秒）后，形成 1 毫秒的停顿。一方面是为了满足第 4 条规则的要求，另一方面也是进行长时间导航计算的需要。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310203328094.png" alt="image-20240310203328094" style="zoom:67%;" />

在这里，您可以清楚地看到，1 号信道 "幸运地 "接收到了 20 个 PRN 代码中的 7 个，在这个信道中，您可以确定导航数据符号变化的时刻（如果有的话）。我注意到，使用信道切换显然会使跟踪复杂化，因为一个信道中的数据接收 "不稳定"。例如，我一直无法将 PLL 配置为在这种模式下工作，在扫描期间只能使用四个 PRN 中的一个 PRN 的数据。

## 导航电文

既然四个通道的数据接收都已实现，我们就可以讨论导航数据的接收了。其来源是相关器 P（提示）通道 I 产生的结果。根据结果的极性，可以认为接收到的数据为 1 或 0。

以某种方式切换信道会使导航数据的提取复杂化。这里的主要任务是跟踪（至少不是很准确）导航比特符号变化的时刻。由于导航比特的变化周期是 20 毫秒的严格倍数，因此有可能与可能的噪声作斗争。在检测到比特的周期性变化后，程序将确定系统时间中发生变化的时刻。这样，通过分析各个 PRN 的跟踪结果和了解当前的系统时间，就可以确定它们所指的是哪个导航位。

由于要确定坐标，我们需要知道导航位变化的确切时间，因此我单独设计了一种算法来计算。让我提醒大家，不准确的原因是 "循环 "相关器的特殊性，它在导航位符号变化的时刻产生不准确的值。该算法要等到收集到的四个相关值的符号变化正好位于数组的中间。然后，通过分析所有四个值和当前相位代码的值，该算法确定真正的符号变化时刻属于第二个值还是第三个值。我不能说该算法绝对可靠--强噪声的存在会导致时间确定的误差。

## 解码导航数据

IS-GPS-200M 文档[4]对导航数据格式进行了详细描述。描述从 "20.3.2 报文结构 "一节开始。现将要点概述如下：

* 数据以单个比特为单位传输，一个比特持续 20 毫秒。
* 30 个比特组成一个字。一个字总是以校验和结束，校验和占 6 个比特。
* 10 个字组成一个子帧。它们分为 5 种类型，每种类型包含各自的数据。一个子帧的传输时间为 6 秒。
* 5 个子帧组成一个帧。一个帧的传输时间为 30 秒。这足以传输当前时间和星历表（单颗卫星的轨道数据）。
* 25 个帧构成一个完整的导航电文。它持续 12.5 分钟，包含所有卫星的历书数据。

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310203520722.png" alt="image-20240310203520722" style="zoom:67%;" />

每个子帧都以一个特殊的 TLM 字开始，该字以 8 比特前言（报头）开始。这可以用来检测子帧的开始，但必须随后检查各个字的校验和。校验和有点混乱，要检查它还需要知道前一个字的 29/30 比特。

子框架组装完成后，就可以进行解码了。在这里，我也使用了 GNSS-SDRLIB 项目的部分代码，在那里，我使用了 RTKLIB 的部分代码。子帧 1 包含卫星信息和一些时间校正系数。此外，这里还传送当前 GPS 周的编号。在子帧 2/3 中传输星历数据。在子帧 4/5 中传输历书数据和一些附加系数。在我的项目中，使用的卫星数量由用户输入，因此不需要历书数据。从这些子帧以及其他子帧中，程序只提取当前 TOW 时间的值。

如上文所述，所使用的 Costas Loop 控制器对 180 度相位变化不敏感。这意味着接收数据的极性可以反转（这只能靠运气）。此外，如果信号电平较低，PLL 可能无法保持相位，接收到的数据极性就会立即反转。因此，我们必须将数据解码方法复杂化，并检查接收到的数据是否反相。

## 显示接收机当前状态

由于接收器 "同时 "处理来自四颗卫星的数据，而重要参数的数量相当多，因此我决定显示当前的主要接收参数。这里一切都很简单--微控制器在内存中形成文本数据并将其发送到 UART，我使用的是 DMA。PC 接收这些数据并将其输出到终端仿真器。看起来是这样的：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310203724093.png" alt="image-20240310203724093" style="zoom:67%;" />

您可以看到卫星编号、接收模式、信噪比（简单计算为 I/Q）、接收频率、编码相位、接收字数和子帧。

## 伪距生成

现在已经接收到导航数据，接收器正在跟踪来自四颗卫星的信号。但如何确定与卫星的距离呢？

在 GPS 系统中，所有卫星严格按照同一时间开始发送子帧，但由于卫星到接收器的距离不同，它们到达接收器的时间也略有不同。延迟时间通常在 65-83 毫秒之间。直接测量延迟是不可能的，因此采用相对测量法。最近卫星的子帧获取时间（它将是所有卫星中最小的）取零，其余的延迟时间从它开始计算（这颗卫星被视为 "参考 "卫星）：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310203809304.png" alt="image-20240310203809304" style="zoom: 80%;" />

此外，为了方便起见，还可以将 68.802ms 添加到获得的数值中，这样时间和距离的数值就会更接近真实值。将得到的时间值乘以光速，我们就得到了到卫星的距离值（伪距）。之所以称为伪距，是因为它们包含各种误差。这些误差的补偿应由定位算法处理。

本项目中的系统时间是以 1 毫秒为单位离散计算的（在 SPI 数据到达时）。这对于充分定位来说太小了--1 毫秒相当于约 300 公里的距离。因此，为了提高精确度，使用了代码相位数据--其测量精确度为 1/16 码片--即大约 18 米。有关伪距计算的更多信息，请参阅此处和本书[3]。

## 将测量结果传输到 PC

从接收器收到计算出的当前伪距及其对应的时间后，我决定将这些数据传输到个人电脑，以检查其性能--就像在 GNSS-SDRLIB 中所做的那样。为了传输这些数据，我使用了 RTCM 3 协议，该协议主要用于传输校正信息，以提高导航系统的精度，但也适用于传输当前接收器的测量数据。

在这个项目中，我使用 RTKLIB 项目的代码来生成 RTCM 信息。为了减小代码的大小，我对代码库进行了大量修改。传输的报文有两种：一种是伪距测量值及其对应的时间（观测值）--ID 1075，另一种是星历数据--ID 1019。由于导航数据的解码也是基于 RTKLIB 代码，因此无需在程序中对星历数据重新编码。微控制器通过 DMA 将打包数据发送至 UART。

然后，这些数据就可以在 PC 上以与前一篇文章中相同的方式进行处理 - 使用 RTKLIB 可以确定当前坐标。结果如下：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310203927728.png" alt="image-20240310203927728" style="zoom:67%;" />

可以看到，坐标偏差相当大：±80 米。这与我之前收到的 GNSS-SDR 结果有些类似。遗憾的是，我无法将天线放置在空旷的天空下，只能安装在阳台的窗户上（室内）。同时，在家里也能看到地平线。

事实证明，接收器对信号电平的要求很高--如果信号电平很弱，它就无法通过信号捕获（获取）阶段。这就导致必须等到卫星进入天线提供信号接收的区域。因此，HDOP 值一直居高不下。

目前的算法可以每 17 毫秒获得一次伪距。如果我们将代码相位测量取平均值呢？我试过了，得到了这样的差异（100 个点的平均值，约为 400 毫秒）：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204000391.png" alt="image-20240310204000391" style="zoom:67%;" />

可以看出，平均值可将噪声水平降低约 2 倍。在所有情况下，您都可以看到，相对于天线的真实坐标（通常为几十米），光斑中心明显偏移。我还没有分析这与什么有关。

## 在 MCU 上实现定位

为了能将设计成果称作正式的 GPS 接收机，有必要将坐标计算转移到微控制器上。我考虑了几种现成算法的变体，但最终还是决定使用相同的 RTKLIB。您可以在这里和 [3] 中了解接收器是如何计算坐标的。

首先，我将定位所需的所有功能移到了一个文件中，并删除了对其他导航系统和频率的支持。接下来，我在微控制器上测试了这个库。不出所料，计算时间相当长--长达 30 毫秒（测试运行，接收器本身不运行），因此我不得不重新设计一些函数。我将它们变成了迭代函数，这是因为长时间的运算往往只与一颗卫星有关，而与其他数据无关。此外，坐标计算算法本身也在最后阶段进行迭代。因此，任何定位功能所需的时间都有可能少于 1 毫秒。由于计算窗口每 17 毫秒出现一次，因此确定一个坐标需要 350-700 毫秒。

计算出的坐标和当前时间与接收状态一起显示。为了更加清晰，我使用伪图形来显示坐标散点图：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204132190.png" alt="image-20240310204132190" style="zoom: 50%;" />

启用码相位平均后，坐标散点如下所示：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204203391.png" alt="image-20240310204203391" style="zoom:67%;" />

该计划的最终结构方案如下：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204227598.png" alt="image-20240310204227598" style="zoom:67%;" />

由此产生的接收器本身看起来并不起眼：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204247883.png" alt="image-20240310204247883" style="zoom: 50%;" />

## ESP32 上的 GPS 接收器

我在 ESP32-2432S024C 上组装了一块调试板。该板有一个对角线为 2.4 英寸的 320x240 液晶显示屏和一个电容式触摸屏。价格约为 10 美元。它看起来是这样的：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204333660.png" alt="image-20240310204333660" style="zoom:50%;" />

这是一种廉价的黄色显示屏。我有个想法，想在这块电路板上制作一个 GPS 接收器。MCU 规格（ESP32-WROOM-32 模块）：

* 2 个 Tensilica Xtensa LX6 内核，32 位
* 主频：240MHz
* Flash：4 Mbytes
* DRAM - 520 千字节（但用户只能使用 320 千字节的 DRAM）

由于微控制器是双核的，其中一个内核可以完全用于图形处理。为了给微控制器编程，我使用了 ESP-IDF 框架和 VS Code 插件。

## 从 SPI 接收数据

遗憾的是，上述电路板用于连接外部设备的空闲引脚太少，只有 3 个。根据文档，SPI 可以重新分配到任何引脚上，三个引脚（CLK/DATA/CSn）对我来说就足够了，但这种模式在高频率下可能无法正常工作。因此，我决定使用文档中推荐的 SPI 线路。在这块电路板上，它们与 microSD 卡座相连。如果不使用卡，它们也可以正常使用。我将必要的导线焊接到 ESP32 模块的焊盘上。重要的是要记住 CSn 线--它必须接地，这样 SPI 从站才能接收数据。看起来是这样的：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204553583.png" alt="image-20240310204553583" style="zoom: 50%;" />

在将项目移植到 ESP32 时，从 SPI 开始接收数据花费了我最多的时间。这种微控制器和 STM32 一样，允许将 SPI 与 DMA 结合使用。不过，外设模块的可用文档有点少。制造商提供的文档主要强调如何使用 ESP-IDF 中包含的现成高级库。这里有一个问题--用于处理从属 SPI 的库是为接收有限的数据量而设计的。据我所知，它不能配置为连续数据采集。我不得不重新编写该库。

这里使用的 DMA 控制器与 STM32 使用的有很大不同。首先，它一次只能传输 4090 个单位的数据。我很幸运--SPI 工作在 8 位模式下，因此需要传输 16368/8=2046 次才能接收 1 毫秒的数据。如果限制更低，这种情况可能会导致程序更加复杂。

此外，与 STM32 不同的是，DMA 设置的一部分存储在用户的 RAM 中，即称为描述符的特殊结构中。特别是，每个描述符都包含应写入数据的地址信息、数据量以及指向下一个描述符的指针。DMA 处理完一个描述符后，就会转到下一个描述符。在本项目中，程序创建了两个相互引用的描述符。这就是我们组织环形数据记录的方法。

文档中还提到："如果启用了 DMA，则 rx 缓冲区应该字对齐（从 32 位边界开始，长度为 4 字节的倍数）。否则，DMA 可能会以边界对齐的方式错误写入或不写入"。我的数据大小不是 4 字节的倍数，但数据接收正常。如果我必须满足这个条件，我就必须明显地重新设计接收数据的方式。

最终，我编写了一个库，其工作方式与 STM32 相同--两个缓冲区各为 1 毫秒，当缓冲区满时，DMA 会产生中断。

## 数据处理

由于 FreeRTOS 是 ESP-IDF 的一部分，我使用了 RTOS 的功能来处理接收到的数据。要接收 GPS 数据，需要创建两个任务--一个优先级较高的任务和一个优先级较低的任务。优先级较高的任务等待来自 DMA 的中断，一旦接收到中断，就进行数据复制和跟踪。

低优先级任务负责采集算法和计算接收器坐标。由于采用了抢占式多任务处理，因此不再需要将坐标计算分成多次迭代，从而大大提高了这部分代码的可读性。性能也有了明显提高--坐标计算大约需要 20-40 毫秒（不过我认为更高的 MCU 频率也有帮助）。

因此，从 STM32 移植过来的 SDR 代码无需任何修改即可立即运行。

## GUI

我用 LVGL 制作了这个项目的图形用户界面。我之前已经接触过 LVGL，所以并不难。我不得不对程序库做了一些调整--所选的 ESP-IDF / LVGL / 显示器和触摸屏驱动程序组合存在一些兼容性问题。我在 SquareLine Studio 中绘制了图形用户界面设计草图。所有信息都显示在四个屏幕元素上，可通过手势进行切换。

* 第一个屏幕用于输入接收机设置（卫星 PRN 编号以及可选的频率偏移）。
* 第二个屏幕用于显示接收机的当前状态，这里显示的参数与 STM32 控制台中的参数大致相同。
* 第三屏是 I/Q 值分布图。
* 第四屏 - 显示特定坐标的散点图。

程序结构图：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204754501.png" alt="image-20240310204754501" style="zoom: 67%;" />

现场照片：

<img src="https://pic-bed-1316053657.cos.ap-nanjing.myqcloud.com/img/image-20240310204829865.png" alt="image-20240310204829865" style="zoom:50%;" />

基于 STM32 和 ESP32 的接收器操作演示视频：

## 播放记录的 GPS 数据

在测试接收机操作时，能够播放预先录制的信号是非常有用的--这样就不必把天线放在窗户上，不必担心信号质量，也不必等待确定每个卫星频率。在个人电脑上，您只需从光盘中读取数据即可。要在单片机上测试代码，就需要将记录的数据以比特流的形式从个人电脑传输到单片机。我考虑了实现这一任务的不同方案，最后决定使用 FT232H 芯片进行数据回放。甚至还有一个现成的项目 SpiLight，可以通过 SPI 回放数据。但它有一个缺点--数据回放速度太慢，而且最大数据量限制在 64 KB。因此，我不得不对 SpiLight 的代码进行了一些调整，结果发布在下面的 STM32 存储库中。

由此产生的最大 SPI 频率为 15MHz，这与写入数据的 16.368MHz 频率不同，但不会对接收器造成问题，只是接收器的系统时间会稍慢一些。由于 FT232H 的限制，每 64KB 数据之间会有一点延迟，但这也不会影响接收器，因为数据本身并没有损坏。

如果您使用以前记录的数据生成 RTCM 数据流，您可能会发现 RTKLIB（rtknavi.exe 程序）无法处理它。据我所知，这是由于 RTCM 数据不包含当前 GPS 周的信息，而 rtknavi 是通过 PC 系统时钟计算其值的。因此，为了测试 RTCM 数据处理情况，我额外使用了 RunAsDate 程序，该程序将当前时间信息替换为 rtknavi。

## 成果

最后，我用微控制器制作了一个 GPS 接收器。该接收器可以接收来自 4 颗卫星的信号，实时处理这些信号并确定自己的坐标。但这纯粹是一个演示项目，对于实际应用来说，它有太多的缺点--确定坐标的精确度低，需要手动指定卫星编号，搜索卫星频率的时间太长，采集需要卫星信号足够强。然而，接收器却能正常工作，我设法在没有 FPGA 或功能强大的 DSP 的情况下做到了这一点。起初我担心在 STM32F4 上无法制作接收器，需要功能更强大的 STM32H7。

项目源代码：

* https://github.com/iliasam/STM32F4_SDR_GPS
* https://github.com/iliasam/ESP32_SDR_GPS

## 参考链接

1. 基于 FPGA 的自制 GPS 接收机项目：[Homemade GPS Receiver](http://www.aholme.co.uk/GPS/Main.htm)。这里的射频前端也是自制的。
2. 90 年代一个非常酷的 GPS 接收机项目，采用模拟芯片、普通数字芯片和 MC68010 作为 CPU：[A homemade receiver for GPS & GLONASS satellites](https://lea.hamradio.si/~s53mv/navsats/theory.html)。这里还详细介绍了全球导航卫星系统的理论。
3. 一本关于制作全球导航卫星系统软件接收器的书：[A Software-Defined GPS and Galileo Receiver: A Single-Frequency Approach](https://www.ocf.berkeley.edu/~marsy/resources/gnss/A Software-Defined GPS and Galileo Receiver.pdf)
4. [IS-GPS-200M](https://www.gps.gov/technical/icwg/IS-GPS-200M.pdf)：描述了 GPS 系统传输的信号。